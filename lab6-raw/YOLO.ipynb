{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "JkJCvRyKcDWG"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CMPT 743 - Spring 2024\n",
        "# Object detection with YOLO\n",
        "\n",
        "__content creator:__ Aryan Mikaeili"
      ],
      "metadata": {
        "id": "iA7hBEyXF9Y0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Import libraries\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.ops import batched_nms\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import pandas as pd\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "ok0uO7sSF8np",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Utils\n",
        "\n",
        "def intersection_over_union(boxes_preds, boxes_labels, box_format = 'midpoint'):\n",
        "\n",
        "    if box_format == 'midpoint':\n",
        "        box1_x1 = boxes_preds[..., 0:1] - boxes_preds[..., 2:3] / 2\n",
        "        box1_y1 = boxes_preds[..., 1:2] - boxes_preds[..., 3:4] / 2\n",
        "        box1_x2 = boxes_preds[..., 0:1] + boxes_preds[..., 2:3] / 2\n",
        "        box1_y2 = boxes_preds[..., 1:2] + boxes_preds[..., 3:4] / 2\n",
        "        box2_x1 = boxes_labels[..., 0:1] - boxes_labels[..., 2:3] / 2\n",
        "        box2_y1 = boxes_labels[..., 1:2] - boxes_labels[..., 3:4] / 2\n",
        "        box2_x2 = boxes_labels[..., 0:1] + boxes_labels[..., 2:3] / 2\n",
        "        box2_y2 = boxes_labels[..., 1:2] + boxes_labels[..., 3:4] / 2\n",
        "    elif box_format == 'corners':\n",
        "        box1_x1 = boxes_preds[..., 0:1]\n",
        "        box1_y1 = boxes_preds[..., 1:2]\n",
        "        box1_x2 = boxes_preds[..., 2:3]\n",
        "        box1_y2 = boxes_preds[..., 3:4]\n",
        "        box2_x1 = boxes_labels[..., 0:1]\n",
        "        box2_y1 = boxes_labels[..., 1:2]\n",
        "        box2_x2 = boxes_labels[..., 2:3]\n",
        "        box2_y2 = boxes_labels[..., 3:4]\n",
        "    else:\n",
        "        raise ValueError('Unknown box format.')\n",
        "\n",
        "    x1 = torch.max(box1_x1, box2_x1)\n",
        "    y1 = torch.max(box1_y1, box2_y1)\n",
        "    x2 = torch.min(box1_x2, box2_x2)\n",
        "    y2 = torch.min(box1_y2, box2_y2)\n",
        "\n",
        "    intersection = (x2 - x1).clamp(0) * (y2 - y1).clamp(0)\n",
        "    box1_area = abs((box1_x2 - box1_x1) * (box1_y2 - box1_y1))\n",
        "    box2_area = abs((box2_x2 - box2_x1) * (box2_y2 - box2_y1))\n",
        "    union = box1_area + box2_area - intersection\n",
        "\n",
        "    return intersection / (union + 1e-6)\n",
        "\n",
        "def convert_cellboxes(boxes):\n",
        "    #boxes: torch tensor (B, S, S, 30)\n",
        "    bboxes1 = boxes[..., 21:25]\n",
        "    bboxes2 = boxes[..., 26:30]\n",
        "    scores = torch.cat((boxes[..., 20].unsqueeze(0), boxes[..., 25].unsqueeze(0)), dim = 0)\n",
        "    best_box = scores.argmax(0).unsqueeze(-1)\n",
        "    best_boxes = bboxes1 * (1 - best_box) + best_box * bboxes2\n",
        "    x_cell, y_cell = best_boxes[..., 0:1], best_boxes[..., 1:2]\n",
        "    width_cell, height_cell = best_boxes[..., 2:3], best_boxes[..., 3:4]\n",
        "\n",
        "    cell_indices = torch.arange(7).repeat(7, 1).unsqueeze(-1).to(boxes.device)\n",
        "\n",
        "    x = (x_cell + cell_indices) / 7\n",
        "    y = (y_cell + cell_indices.permute(1, 0, 2)) / 7\n",
        "\n",
        "    width = width_cell / 7\n",
        "    height = height_cell / 7\n",
        "\n",
        "    converted_boxes = torch.cat((x, y, width, height), dim = -1)\n",
        "    predicted_class = boxes[..., :20].argmax(-1).unsqueeze(-1)\n",
        "    best_confidence = torch.max(boxes[..., 20], boxes[..., 25]).unsqueeze(-1)\n",
        "\n",
        "    converted_pred = torch.cat((predicted_class, best_confidence, converted_boxes), dim = -1)\n",
        "\n",
        "    return converted_pred\n",
        "\n",
        "def cellboxes_to_boxes(boxes, S = 7):\n",
        "    #boxes: torch tensor (B, S, S, 30)\n",
        "    converted_pred = convert_cellboxes(boxes)\n",
        "\n",
        "    B = converted_pred.size(0)\n",
        "\n",
        "    all_bboxes = converted_pred.reshape(B, S * S, -1)\n",
        "\n",
        "    return all_bboxes\n",
        "\n",
        "def non_max_suppression(bboxes, iou_threshold = 0.5, confidence_threshold = 0.4):\n",
        "    #bboxes: (N, 6)\n",
        "    #boxes: (N, 6) -> (N2, 4)\n",
        "\n",
        "    #prune boxes that have confidence < threshold\n",
        "    bboxes = bboxes[bboxes[:, 1] > confidence_threshold]\n",
        "    bboxes_coords = bboxes[:, 2:6]\n",
        "    scores = bboxes[:, 1]\n",
        "    class_bboxes = bboxes[:, 0]\n",
        "\n",
        "    # convert xywh to x1y1x2y2\n",
        "    x1 = bboxes_coords[:, 0] - bboxes_coords[:, 2] / 2\n",
        "    y1 = bboxes_coords[:, 1] - bboxes_coords[:, 3] / 2\n",
        "    x2 = bboxes_coords[:, 0] + bboxes_coords[:, 2] / 2\n",
        "    y2 = bboxes_coords[:, 1] + bboxes_coords[:, 3] / 2\n",
        "\n",
        "    bboxes_coords = torch.stack((x1, y1, x2, y2)).T\n",
        "\n",
        "    keep = batched_nms(bboxes_coords, scores, class_bboxes, iou_threshold=iou_threshold)\n",
        "\n",
        "    return bboxes[keep]\n",
        "\n",
        "def visualize_images(images, row_size):\n",
        "    #images: list of PIL images\n",
        "    #row_size: number of images per row\n",
        "    #returns: PIL image\n",
        "\n",
        "    num_images = len(images)\n",
        "    col_size = num_images // row_size\n",
        "    if num_images % row_size != 0:\n",
        "        col_size += 1\n",
        "\n",
        "    image_size = images[0].size[0]\n",
        "    canvas = Image.new('RGB', (image_size * row_size, image_size * col_size))\n",
        "    for i, image in enumerate(images):\n",
        "        canvas.paste(image, (image_size * (i % row_size), image_size * (i // row_size)))\n",
        "    return canvas\n",
        "\n",
        "def draw_boxes(image, gt_boxes, pred_boxes):\n",
        "    image = ((image.permute(1, 2, 0).cpu().detach().numpy() + 1) * 127.5).astype('uint8')\n",
        "    image_pred = image.copy()\n",
        "    image_gt = image.copy()\n",
        "    for box in gt_boxes:\n",
        "        x, y, w, h = box[3:]\n",
        "        x1 = int((x - w / 2) * 448)\n",
        "        y1 = int((y - h / 2) * 448)\n",
        "        x2 = int((x + w / 2) * 448)\n",
        "        y2 = int((y + h / 2) * 448)\n",
        "\n",
        "        image_gt = cv2.rectangle(image_gt, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "\n",
        "    for box in pred_boxes:\n",
        "        x, y, w, h = box[3:]\n",
        "        x1 = int((x - w / 2) * 448)\n",
        "        y1 = int((y - h / 2) * 448)\n",
        "        x2 = int((x + w / 2) * 448)\n",
        "        y2 = int((y + h / 2) * 448)\n",
        "\n",
        "        image_pred = cv2.rectangle(image_pred, (x1, y1), (x2, y2), (0, 0, 255), 2)\n",
        "\n",
        "    image_gt = Image.fromarray(image_gt)\n",
        "    image_pred = Image.fromarray(image_pred)\n",
        "\n",
        "    image = visualize_images([image_gt, image_pred], 2)\n",
        "\n",
        "    return image\n",
        "\n",
        "def draw_boxes_with_labels(image, pred_labels, gt_labels, pred_classes, gt_classes):\n",
        "    image = ((image.permute(1, 2, 0).cpu().detach().numpy() + 1) * 127.5).astype('uint8')\n",
        "    image_pred = image.copy()\n",
        "    image_gt = image.copy()\n",
        "    for i, box in enumerate(gt_labels):\n",
        "        x, y, w, h = box[3:]\n",
        "        x1 = int((x - w / 2) * 448)\n",
        "        y1 = int((y - h / 2) * 448)\n",
        "        x2 = int((x + w / 2) * 448)\n",
        "        y2 = int((y + h / 2) * 448)\n",
        "\n",
        "        image_gt = cv2.rectangle(image_gt, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "        cv2.putText(image_gt, gt_classes[i], (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 2.5, (0, 255, 0), 2)\n",
        "\n",
        "    for i, box in enumerate(pred_labels):\n",
        "        x, y, w, h = box[3:]\n",
        "        x1 = int((x - w / 2) * 448)\n",
        "        y1 = int((y - h / 2) * 448)\n",
        "        x2 = int((x + w / 2) * 448)\n",
        "        y2 = int((y + h / 2) * 448)\n",
        "\n",
        "        image_pred = cv2.rectangle(image_pred, (x1, y1), (x2, y2), (0, 0, 255), 2)\n",
        "        cv2.putText(image_pred, pred_classes[i], (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 2.5, (0, 0, 255), 2)\n",
        "\n",
        "    image_gt = Image.fromarray(image_gt)\n",
        "    image_pred = Image.fromarray(image_pred)\n",
        "    image = visualize_images([image_gt, image_pred], 2)\n",
        "\n",
        "    return image\n",
        "\n"
      ],
      "metadata": {
        "id": "wKWQe9C7WYRj",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title TODO: Intersection over Union (15 minutes)\n",
        "\n",
        "def intersection_over_union(boxes_preds, boxes_labels, box_format = 'midpoint'):\n",
        "    # boxes_preds: (..., 4)\n",
        "    # boxes_labels: (..., 4)\n",
        "    #box format: format of input boxes\n",
        "    if box_format == 'midpoint':\n",
        "        box1_x1 = boxes_preds[..., 0:1] - boxes_preds[..., 2:3] / 2\n",
        "        box1_y1 = boxes_preds[..., 1:2] - boxes_preds[..., 3:4] / 2\n",
        "        box1_x2 = boxes_preds[..., 0:1] + boxes_preds[..., 2:3] / 2\n",
        "        box1_y2 = boxes_preds[..., 1:2] + boxes_preds[..., 3:4] / 2\n",
        "        box2_x1 = boxes_labels[..., 0:1] - boxes_labels[..., 2:3] / 2\n",
        "        box2_y1 = boxes_labels[..., 1:2] - boxes_labels[..., 3:4] / 2\n",
        "        box2_x2 = boxes_labels[..., 0:1] + boxes_labels[..., 2:3] / 2\n",
        "        box2_y2 = boxes_labels[..., 1:2] + boxes_labels[..., 3:4] / 2\n",
        "    elif box_format == 'corners':\n",
        "        box1_x1 = boxes_preds[..., 0:1]\n",
        "        box1_y1 = boxes_preds[..., 1:2]\n",
        "        box1_x2 = boxes_preds[..., 2:3]\n",
        "        box1_y2 = boxes_preds[..., 3:4]\n",
        "        box2_x1 = boxes_labels[..., 0:1]\n",
        "        box2_y1 = boxes_labels[..., 1:2]\n",
        "        box2_x2 = boxes_labels[..., 2:3]\n",
        "        box2_y2 = boxes_labels[..., 3:4]\n",
        "    else:\n",
        "        raise ValueError('Unknown box format.')\n",
        "\n",
        "    #TODO: find (x1, y1) and (x2, y2) of\n",
        "    x1 = ...\n",
        "    y1 = ...\n",
        "    x2 = ...\n",
        "    y2 = ...\n",
        "\n",
        "    #TODO: find intersection area\n",
        "    intersection = ...\n",
        "\n",
        "    #TODO: find union area\n",
        "    union = ...\n",
        "\n",
        "    return intersection / (union + 1e-6)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "T94pYktx0rdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Check IoU implementation\n",
        "\n",
        "boxes1 = torch.tensor([[0.5, 0.5, 0.6, 0.3], [0.2, 0.2, 0.1, 0.1]])\n",
        "boxes2 = torch.tensor([[0.5, 0.5, 0.3, 0.6], [0.6, 0.6, 0.2, 0.3]])\n",
        "assert (torch.abs(intersection_over_union(boxes1,  boxes2).reshape(2, ) - torch.tensor([0.3333, 0.])) < 1e-4).all()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "oqvf363v3BZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download and config dataset"
      ],
      "metadata": {
        "id": "gYjLN0jgOfsC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download dataset\n",
        "\n",
        "import zipfile\n",
        "\n",
        "!gdown --id 1HIPUz1aguNp8RTSx_H-f7z9_OHOu35n5\n",
        "\n",
        "zip_file = zipfile.ZipFile('small_data.zip')\n",
        "zip_file.extractall()\n",
        "\n",
        "zip_file.close()"
      ],
      "metadata": {
        "id": "_eQoCalaq1nx",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Dataset class\n",
        "\n",
        "class VOCDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data_dir, split = 'train', S=7, B=2, C=20, transform=None):\n",
        "        csv_file = os.path.join(data_dir, '100examples.csv')\n",
        "        self.annotations = pd.read_csv(csv_file)\n",
        "        self.image_dir = os.path.join(data_dir, 'images')\n",
        "        self.label_dir = os.path.join(data_dir, 'labels')\n",
        "        self.transform = transform\n",
        "        self.S = S\n",
        "        self.B = B\n",
        "        self.C = C\n",
        "        self.class_names = [\n",
        "            \"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\", \"bus\", \"car\", \"cat\",\n",
        "            \"chair\", \"cow\", \"diningtable\", \"dog\", \"horse\", \"motorbike\", \"person\",\n",
        "            \"pottedplant\", \"sheep\", \"sofa\", \"train\", \"tvmonitor\"\n",
        "            ]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.annotations)\n",
        "    def __getitem__(self, idx):\n",
        "        label_path = os.path.join(self.label_dir, self.annotations.iloc[idx, 1])\n",
        "        boxes = []\n",
        "        with open(label_path) as f:\n",
        "            for label in f.readlines():\n",
        "                class_label, x, y, width, height = [\n",
        "                    float(x) if float(x) != int(float(x)) else int(x)\n",
        "                    for x in label.replace('\\n', '').split()\n",
        "                ]\n",
        "                boxes.append([class_label, x, y, width, height])\n",
        "        img_path = os.path.join(self.image_dir, self.annotations.iloc[idx, 0])\n",
        "        image = Image.open(img_path)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        label_matrix = torch.zeros((self.S, self.S, self.C + 5 * self.B))\n",
        "        for box in boxes:\n",
        "            class_label, x, y, width, height = box\n",
        "            class_label = int(class_label)\n",
        "            i, j = int(self.S * y), int(self.S * x)\n",
        "            x_cell, y_cell = self.S * x - j, self.S * y - i\n",
        "            width_cell, height_cell = (\n",
        "                width * self.S,\n",
        "                height * self.S\n",
        "            )\n",
        "            if label_matrix[i, j, 20] == 0:\n",
        "                label_matrix[i, j, 20] = 1\n",
        "                box_coordinates = torch.tensor(\n",
        "                    [x_cell, y_cell, width_cell, height_cell]\n",
        "                )\n",
        "                label_matrix[i, j, 21:25] = box_coordinates\n",
        "                label_matrix[i, j, class_label] = 1\n",
        "        return image, label_matrix\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "abAK3WxlGgKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **TODO**: Initialize dataset (5 minutes)\n",
        "# TODO: Initialize the dataset with the correct transformations\n",
        "\n",
        "# Transformation should:\n",
        "# 1. resize the image to (448, 448)\n",
        "# 2. convert images to torch tensors with values (-1, 1)\n",
        "# Use transform.Compose([])\n",
        "\n",
        "data_dir = './small_data'\n",
        "transform = ...\n",
        "\n",
        "train_dataset = VOCDataset(data_dir = data_dir, transform = transform)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "KGUOcCK_HJs1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **TODO**: Visualize images with bounding boxes (20 minutes)\n",
        "\n",
        "# TODO: get an item from the dataset\n",
        "sample_image, sample_label = ...\n",
        "\n",
        "# TODO: convert image to numpy array with shape (448, 448 , 3) in the range (0, 255) and type uint8\n",
        "image = ...\n",
        "\n",
        "H, W = image.shape[:2]\n",
        "# TODO: 1. find the cells in the image grid where there is a bounding box\n",
        "# 2. get the coordinates of the boxes\n",
        "# Hint: Use torch.where\n",
        "\n",
        "box_idx = ...\n",
        "boxes = ...\n",
        "\n",
        "#TODO: iterate over the boxes and Compute the global coordinates of the boxes\n",
        "image = image.copy()\n",
        "for i in range(len(boxes)):\n",
        "  x_cell, y_cell = ...\n",
        "  width_cell, height_cell = ...\n",
        "\n",
        "  x = ...\n",
        "  y = ...\n",
        "  width = ...\n",
        "  height = ...\n",
        "  image = cv2.rectangle(image, (int(x - width / 2), int(y - height / 2)),\n",
        "                  (int(x + width / 2), int(y + height / 2)), (0, 0, 255), 2)\n",
        "\n",
        "\n",
        "\n",
        "Image.fromarray(image)\n"
      ],
      "metadata": {
        "id": "8DYx9iy8Iv0D",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Config Model"
      ],
      "metadata": {
        "id": "zlJY_La7PBOx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **TODO**: CNN Block (10 minutes)\n",
        "\n",
        "# A CNN Block consists of:\n",
        "## 1. A convolution layer\n",
        "## 2. A batch normalization layer\n",
        "## 3. A activation function. Use LeakyReLU with slop 0.1\n",
        "class CNNBlock(nn.Module):\n",
        "  def __init__(self, in_channel, out_channel, **kwargs):\n",
        "      #in_channel: input channels of the convolution layer\n",
        "      #out_channel: output channels of the convolution layer\n",
        "      #kwargs: contains kernel size, stride, padding\n",
        "\n",
        "      super(CNNBlock, self).__init__()\n",
        "      self.model = nn.Sequential(\n",
        "          ...\n",
        "      )\n",
        "\n",
        "  def forward(self, x):\n",
        "    ..."
      ],
      "metadata": {
        "cellView": "form",
        "id": "XlTQkAnWFNdg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **TODO**: YOLO Network (30 minutes)\n",
        "\n",
        "\n",
        "# architecure of the YOLO network\n",
        "# Tuple :(kernel size, out channels, stirde, padding)\n",
        "# 'M': 2x2 max pooling\n",
        "# List: contains Tuples with the same convention mentioned. The last element is number of repetitions\n",
        "\n",
        "architecture_config = [\n",
        "    (7, 64, 2, 3),\n",
        "    'M',\n",
        "    (3, 192, 1, 1),\n",
        "    'M',\n",
        "    (1, 128, 1, 0),\n",
        "    (3, 256, 1, 1),\n",
        "    (1, 256, 1, 0),\n",
        "    (3, 512, 1, 1),\n",
        "    'M',\n",
        "    [(1, 256, 1, 0), (3, 512, 1, 1), 4],\n",
        "    (1, 512, 1, 0),\n",
        "    (3, 1024, 1, 1),\n",
        "    'M',\n",
        "    [(1, 512, 1, 0), (3, 1024, 1, 1), 2],\n",
        "    (3, 1024, 1, 1),\n",
        "    (3, 1024, 2, 1),\n",
        "    (3, 1024, 1, 1),\n",
        "    (3, 1024, 1, 1),\n",
        "]\n",
        "\n",
        "class YOLONet(nn.Module):\n",
        "  def __init__(self, in_channel = 3, **kwargs):\n",
        "    super(YOLONet, self).__init__()\n",
        "    self.in_channel = in_channel\n",
        "    self.darknet = self._create_conv_layers()\n",
        "    self.fcs = self._create_fcs(**kwargs)\n",
        "\n",
        "  def forward(self, x):\n",
        "    #TODO\n",
        "    ...\n",
        "\n",
        "  def _create_conv_layers(self):\n",
        "      #TODO: create Conv layers from the architecture_config list\n",
        "      ...\n",
        "\n",
        "  def _create_fcs(self, split_size, num_boxes, num_classes):\n",
        "    S, B, C = split_size, num_boxes, num_classes\n",
        "    return nn.Sequential(\n",
        "        nn.Linear(1024 * S * S, 1024),\n",
        "        nn.LeakyReLU(0.1),\n",
        "        nn.Linear(1024, S * S * (C + B * 5)),\n",
        "    )\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "efU0LNVqQhw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Test model correctness\n",
        "\n",
        "model = YOLONet(split_size=7, num_boxes=2, num_classes=20)\n",
        "input = torch.randn(4, 3, 448, 448)\n",
        "out = model(input)\n",
        "\n",
        "assert out.shape == (4, 7 * 7 * (20 + 2 * 5))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "3kr5Grv5SmhX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loss Function"
      ],
      "metadata": {
        "id": "oLHpVUIQWJk8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Define loss function\n",
        "class YoloLoss(nn.Module):\n",
        "    def __init__(self, S = 7, B = 2, C = 20):\n",
        "        super(YoloLoss, self).__init__()\n",
        "        self.mse = nn.MSELoss(reduction = 'sum')\n",
        "        self.S = S\n",
        "        self.B = B\n",
        "        self.C = C\n",
        "        self.lambda_noobj = 0.5\n",
        "        self.lambda_coord = 5\n",
        "\n",
        "    def forward(self, predictions, target):\n",
        "        predictions = predictions.reshape(-1, self.S, self.S, self.C + self.B * 5)\n",
        "\n",
        "        iou_b1 = intersection_over_union(predictions[..., 21:25], target[..., 21:25])\n",
        "        iou_b2 = intersection_over_union(predictions[..., 26:30], target[..., 21:25])\n",
        "        ious = torch.cat([iou_b1.unsqueeze(0), iou_b2.unsqueeze(0)], dim = 0)\n",
        "\n",
        "        iou_maxes, bestbox = torch.max(ious, dim = 0)\n",
        "        exists_box = target[..., 20].unsqueeze(3)\n",
        "\n",
        "        # ======================== #\n",
        "        #   FOR BOX COORDINATES    #\n",
        "        # ======================== #\n",
        "\n",
        "        box_predictions = exists_box * (\n",
        "            (\n",
        "                bestbox * predictions[..., 26:30]\n",
        "                + (1 - bestbox) * predictions[..., 21:25]\n",
        "            )\n",
        "        )\n",
        "\n",
        "        box_targets = exists_box * target[..., 21:25]\n",
        "\n",
        "        box_predictions[..., 2:4] = torch.sign(box_predictions[..., 2:4]) * torch.sqrt(\n",
        "            torch.abs(box_predictions[..., 2:4] + 1e-6)\n",
        "        )\n",
        "\n",
        "        box_targets[..., 2:4] = torch.sqrt(box_targets[..., 2:4])\n",
        "\n",
        "        box_loss = self.mse(\n",
        "            torch.flatten(box_predictions, end_dim = -2),\n",
        "            torch.flatten(box_targets, end_dim = -2),\n",
        "        )\n",
        "\n",
        "        # ==================== #\n",
        "        #   FOR OBJECT LOSS    #\n",
        "        # ==================== #\n",
        "\n",
        "        pred_box = (\n",
        "            bestbox * predictions[..., 25:26] + (1 - bestbox) * predictions[..., 20:21]\n",
        "        )\n",
        "\n",
        "        object_loss = self.mse(\n",
        "            torch.flatten(exists_box * pred_box),\n",
        "            torch.flatten(exists_box * target[..., 20:21]),\n",
        "        )\n",
        "\n",
        "        # ======================= #\n",
        "        #   FOR NO OBJECT LOSS    #\n",
        "        # ======================= #\n",
        "\n",
        "        no_object_loss = self.mse(\n",
        "            torch.flatten((1 - exists_box) * predictions[..., 20:21], start_dim = 1),\n",
        "            torch.flatten((1 - exists_box) * target[..., 20:21], start_dim = 1),\n",
        "        )\n",
        "\n",
        "        no_object_loss += self.mse(\n",
        "            torch.flatten((1 - exists_box) * predictions[..., 25:26], start_dim = 1),\n",
        "            torch.flatten((1 - exists_box) * target[..., 20:21], start_dim = 1),\n",
        "        )\n",
        "\n",
        "        # ================== #\n",
        "        #   FOR CLASS LOSS   #\n",
        "        # ================== #\n",
        "\n",
        "        class_loss = self.mse(\n",
        "            torch.flatten(exists_box * predictions[..., :20], end_dim = -2,),\n",
        "            torch.flatten(exists_box * target[..., :20], end_dim = -2,),\n",
        "        )\n",
        "\n",
        "        loss = (\n",
        "            self.lambda_coord * box_loss  # first two rows in paper\n",
        "            + object_loss  # third row in paper\n",
        "            + self.lambda_noobj * no_object_loss  # forth row\n",
        "            + class_loss  # fifth row\n",
        "        )\n",
        "\n",
        "        return loss"
      ],
      "metadata": {
        "cellView": "form",
        "id": "M_pEkMbPWMAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "JkJCvRyKcDWG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **TODO**: Training script (20 minutes)\n",
        "class Trainer:\n",
        "  def __init__(self):\n",
        "    self.lr = 2e-5\n",
        "    self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    self.batch_size = 8\n",
        "    self.weight_decay = 0.0005\n",
        "\n",
        "    self.epoch = 100\n",
        "    self.num_workers = 0\n",
        "    self.pin_memory = True\n",
        "    self.load_model = False\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((448, 448)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
        "                              std=[0.5, 0.5, 0.5])])\n",
        "    #TODO: Initialize train and test datasets and corresponding DataLoader objects\n",
        "    self.train_dataset = ...\n",
        "    self.trainloader = ...\n",
        "\n",
        "    self.test_dataset = ...\n",
        "    self.testloader = ...\n",
        "\n",
        "    self.model = YOLONet(split_size=7, num_boxes=2, num_classes=20).to(self.device)\n",
        "    #TODO: # Define Adam optimizer with model parameters, learning rate, and weight decay\n",
        "    self.optimizer = ...\n",
        "    self.criterion = YoloLoss()\n",
        "\n",
        "    self.out_dir = './outputs'\n",
        "    os.makedirs(self.out_dir, exist_ok = True)\n",
        "\n",
        "\n",
        "  def run(self):\n",
        "    pbar = tqdm(total = self.epoch, bar_format='{desc}: {percentage:3.0f}% {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]', leave=False)\n",
        "    for epoch in range(self.epoch):\n",
        "        if epoch % 10 == 0:\n",
        "          self.test_step()\n",
        "        mean_loss = self.train_step()\n",
        "        pbar.update(1)\n",
        "        pbar.set_description('Epoch: {}, Mean Loss: {:.4f}'.format(epoch, mean_loss))\n",
        "\n",
        "\n",
        "  def train_step(self):\n",
        "    self.model.train()\n",
        "    mean_loss = []\n",
        "    for batch_idx, (image, label) in enumerate(self.trainloader):\n",
        "        image, label = image.to(self.device), label.to(self.device)\n",
        "        #TODO\n",
        "        ...\n",
        "\n",
        "    return sum(mean_loss) / len(mean_loss)\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def test_step(self):\n",
        "    self.model.eval()\n",
        "    all_pred_boxes = []\n",
        "    all_true_boxes = []\n",
        "    sample_idx = 0\n",
        "    for batch_idx, (image, label) in enumerate(self.testloader):\n",
        "        image, label = image.to(self.device), label.to(self.device)\n",
        "        batch_size = image.shape[0]\n",
        "        preds = self.model(image).reshape(-1, 7, 7, 30)\n",
        "\n",
        "        true_bboxes = cellboxes_to_boxes(label)\n",
        "        pred_bboxes = cellboxes_to_boxes(preds)\n",
        "\n",
        "        for idx in range(batch_size):\n",
        "            pred_bboxes_nms = non_max_suppression(pred_bboxes[idx])\n",
        "            for box in pred_bboxes_nms:\n",
        "                all_pred_boxes.append([sample_idx] + box.tolist())\n",
        "            for box in true_bboxes[idx]:\n",
        "                if box[1] > 0:\n",
        "                    all_true_boxes.append([sample_idx] + box.tolist())\n",
        "            sample_idx += 1\n",
        "\n",
        "    self.visualize_results(all_true_boxes, all_pred_boxes)\n",
        "    return all_pred_boxes, all_true_boxes\n",
        "\n",
        "  def visualize_results(self, preds, gt):\n",
        "      for idx in range(len(self.test_dataset)):\n",
        "          image, _ = self.test_dataset.__getitem__(idx)\n",
        "          pred_classes = [self.test_dataset.class_names[int(box[1])] for box in preds if box[0] == idx]\n",
        "          pred_labels = [box for box in preds if box[0] == idx]\n",
        "          gt_classes = [self.test_dataset.class_names[int(box[1])] for box in gt if box[0] == idx]\n",
        "          gt_labels = [box for box in gt if box[0] == idx]\n",
        "\n",
        "          image = draw_boxes_with_labels(image, pred_labels, gt_labels, pred_classes, gt_classes)\n",
        "          image.save('{}/{}.png'.format(self.out_dir, idx))\n"
      ],
      "metadata": {
        "id": "bfqbTxQEcMYw",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer()\n",
        "trainer.run()"
      ],
      "metadata": {
        "id": "wmTP13uAdy3P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}